### GIT⼯具安装

---

在 Linux 上安装 Git 向来仅需⼀⾏命令即可搞定，因为各式各样的包管理器帮了我们⼤忙，所以对于

CentOS 系统来讲，直接执⾏如下命令即可安装：

```shell
yum install git
```

当然通过这种⽅式安装的 Git 可能不是较新版的 Git ，以我们的实验环境 CentOS 7.4 来说，这种⽅

式安装的 Git 版本为 1.8.3.1 ，不过⼀般来说是够⽤的。



### JDK（JAVA环境）安装

---

#### 1、下载

我这⾥下载的是 jdk-8u161-linux-x64.tar.gz 安装包，并将其直接放在了 root ⽬录下



#### 备注：

卸载已有的OPENJDK（如果有）

如果系统⾃带有 OpenJDK ，可以按照如下步骤提前卸载之。

⾸先查找已经安装的 OpenJDK 包：

```shell
rpm -qa | grep java
```

接下来可以将 java 开头的安装包均卸载即可：

```shell
yum -y remove java-1.7.0-openjdk-1.7.0.141-2.6.10.5.el7.x86_64
yum -y remove java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64
```

#### 2、创建目录解压

1、在 /usr/local/ 下创建 java ⽂件夹并进⼊

```shell
cd /usr/local/
mkdir java
cd java
```

2、将上⾯准备好的 JDK 安装包解压到 /usr/local/java 中即可

```shell
tar -zxvf /root/jdk-8u161-linux-x64.tar.gz -C ./
```

解压完之后， /usr/local/java ⽬录中会出现⼀个 jdk1.8.0_161 的⽬录

#### 3、配置环境变量

编辑 /etc/profile ⽂件，在⽂件尾部加⼊如下 JDK 环境配置即可

```shell
JAVA_HOME=/usr/local/java/jdk1.8.0_161
CLASSPATH=$JAVA_HOME/lib/
PATH=$PATH:$JAVA_HOME/bin
export PATH JAVA_HOME CLASSPATH
```

然后执⾏如下命令让环境变量⽣效

```shell
source /etc/profile
```

#### 4、验证

输⼊如下命令即可检查安装结果：

```shell
java -version
javac
```



### NODE环境安装

---

我这⾥下载的是 node-v14.4.0-linux-x64.tar.xz 安装包，并将其直接放在了 root ⽬录下

#### 1、创建⽬录并解压

a、在 /usr/local/ 下创建 node ⽂件夹并进⼊

```shell
cd /usr/local/
mkdir node
cd node
```

b、将 Node 的安装包解压到 /usr/local/node 中即可

```shell
tar -xJvf /root/node-v14.4.0-linux-x64.tar.xz -C ./
```

#### 2、配置NODE系统环境变量

编辑 ~/.bash_profile ⽂件，在⽂件末尾追加如下信息

```shell
vi ~/.bash_profile

# Nodejs
export PATH=/usr/local/node/node-v14.4.0-linux-x64/bin:$PATH
```

刷新环境变量，使之⽣效即可：

```shell
source ~/.bash_profile
```

#### 3、检查安装结果

```shell
node -v
npm version
npx -v
```



### PYTHON安装

---

CentOS 7.4 默认⾃带了⼀个 Python2.7 环境：

然⽽现在主流都是 Python3 ，所以接下来再装⼀个 Python3 ，打造⼀个共存的环境。



#### 1、准备PYTHON3安装包并解压

```shell
wget https://www.python.org/ftp/python/3.8.3/Python-3.8.3.tgz
```

我这⾥下载的是 Python-3.8.3.tgz 安装包，并将其直接放在了 /root ⽬录下

执⾏如下命令解压之：

```shell
tar zxvf Python-3.8.3.tgz
```

则可以在当前⽬录得到⽂件夹： Python-3.8.3



#### 2、安装相关预备环境

```shell
yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlitedevel readline-devel tk-devel gcc make
```

#### 3、编译并安装

这⾥指定了安装⽬录为 /usr/local/python3 ，有需要可以⾃定义

```shell
cd Python-3.8.3/
./configure prefix=/usr/local/python3
make && make install
```

#### 4、添加软链接

我们还需要将刚刚安装⽣成的⽬录 /usr/local/python3 ⾥的 python3 可执⾏⽂件做⼀份软链接，链

接到 /usr/bin 下，⽅便后续使⽤python3

```shell
ln -s /usr/local/python3/bin/python3 /usr/bin/python3
ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3
```



### MAVEN

#### 1、准备安装包

```shell
wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
```

执⾏命令解压之：

```shell
tar zxvf apache-maven-3.6.3-bin.tar.gz
```



#### 2、配置MAVEN加速镜像源

这⾥配置的是阿⾥云的maven镜像源。

编辑修改 /opt/maven/apache-maven-3.6.3/conf/settings.xml

⽂件，在 <mirrors></mirrors> 标签对⾥添加如下内容即可：

```shell
vi /opt/maven/apache-maven-3.6.3/conf/settings.xml
```

```shell
<mirror>
 <id>alimaven</id>
 <name>aliyun maven</name>
 <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
 <mirrorOf>central</mirrorOf>
</mirror>
```



#### 3、配置环境变量

因为下载的是⼆进制版安装包，所以解压完，配置好环境变量即可使⽤了。

编辑修改 /etc/profile ⽂件，在⽂件尾部添加如下内容，配置 maven 的安装路径

```shell
vi /etc/profile

export MAVEN_HOME=/opt/maven/apache-maven-3.6.3
export PATH=$MAVEN_HOME/bin:$PATH

source /etc/profile
```

接下来执⾏ source /etc/profile 来刷新环境变量，让 maven 环境的路径配置⽣效，



#### 4、检验安装结果

```shell
mvn -v
```

执⾏ mvn –v ，能打印出 maven 版本信息说明安装、配置成功



### MYSQL

#### 1、⾸先准备安装包

这⾥下载的是 mysql-5.7.30-linux-glibc2.12-x86_64.tar.gz 安装包，并将其直接放在了 root⽬录下

```shell
wget https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.30-linux-glibc2.12-x86_64.tar.gz
```

#### 2、卸载系统⾃带的MARIADB（如果有）

如果系统之前⾃带 Mariadb ，可以先卸载之。

⾸先查询已安装的 Mariadb 安装包：

```shell
rpm -qa|grep mariadb

yum -y remove mariadb-libs-5.5.56-2.el7.x86_64
```

#### 3、解压MYSQL安装包

将上⾯准备好的 MySQL 安装包解压到 /usr/local/ ⽬录，并重命名为 mysql

```shell
tar -zxvf /root/mysql-5.7.30-linux-glibc2.12-x86_64.tar.gz -C /usr/local/
mv mysql-5.7.30-linux-glibc2.12-x86_64 mysql
```



#### 4、创建MYSQL⽤户和⽤户组

```shell
groupadd mysql
useradd -g mysql mysql

mkdir /usr/local/mysql/data
```

同时新建 /usr/local/mysql/data ⽬录，后续备⽤



修改MYSQL⽬录的归属⽤户

```shell
chown -R mysql:mysql ./
```



#### 5、准备MYSQL的配置⽂件

在 /etc ⽬录下新建 my.cnf ⽂件，写⼊如下简化配置：

```shell
[mysql]
# 设置mysql客户端默认字符集
default-character-set=utf8
socket=/var/lib/mysql/mysql.sock
[mysqld]
skip-name-resolve
#设置3306端⼝
port = 3306
socket=/var/lib/mysql/mysql.sock
# 设置mysql的安装⽬录
basedir=/usr/local/mysql
# 设置mysql数据库的数据的存放⽬录
datadir=/usr/local/mysql/data
# 允许最⼤连接数
max_connections=200
# 服务端使⽤的字符集默认为8⽐特编码的latin1字符集
character-set-server=utf8
# 创建新表时将使⽤的默认存储引擎
default-storage-engine=INNODB
lower_case_table_names=1
max_allowed_packet=16M
```

同时使⽤如下命令创建 /var/lib/mysql ⽬录，并修改权限：

```shell
mkdir /var/lib/mysql
chmod 777 /var/lib/mysql
```

#### 6、正式开始安装MYSQL

执⾏如下命令正式开始安装：

```shell
cd /usr/local/mysql
./bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data
```

注意：记住上⾯打印出来的 root 的密码，后⾯⾸次登陆需要使⽤

108：ZaF3bjZdqj&7

109：tEwvHlBd!1QP

60：gihW*zkOH8;H



#### 7、复制启动脚本到资源⽬录

执⾏如下命令复制：

```shell
cp ./support-files/mysql.server /etc/init.d/mysqld
```

并修改 /etc/init.d/mysqld ，修改其 basedir 和 datadir 为实际对应⽬录：

```shell
vi /etc/init.d/mysqld

basedir=/usr/local/mysql
datadir=/usr/local/mysql/data
```



#### 8、设置MYSQL系统服务并开启⾃启

⾸先增加 mysqld 服务控制脚本执⾏权限：

```shell
chmod +x /etc/init.d/mysqld
```

同时将 mysqld 服务加⼊到系统服务：

```shell
chkconfig --add mysqld
```

最后检查 mysqld 服务是否已经⽣效即可：

```shell
chkconfig --list mysqld
```

在2、3、4、5运⾏级别随系统启动⽽⾃动启动，以后可以直接使

⽤ service 命令控制 mysql 的启停。



#### 9、启动MYSQLD

```shell
service mysqld start
```



#### 10、将 MYSQL 的 BIN ⽬录加⼊ PATH 环境变量

这样⽅便以后在任意⽬录上都可以使⽤ mysql 提供的命令。

编辑 ~/.bash_profile ⽂件，在⽂件末尾处追加如下信息:

```shell
vi ~/.bash_profile

export PATH=$PATH:/usr/local/mysql/bin
```

最后执⾏如下命令使环境变量⽣效

```shell
source ~/.bash_profile
```



#### 11、⾸次登陆MYSQL

以 root 账户登录 mysql ，使⽤上⽂安装完成提示的密码进⾏登⼊

```shell
mysql -u root -p
```



#### 12、接下来修改ROOT账户密码

在mysql的命令⾏执⾏如下命令即可，密码可以换成你想⽤的密码即可：

```shell
mysql> alter user user() identified by "111111";
mysql> flush privileges;
```



#### 13、设置远程主机登录

```shell
mysql> use mysql;
mysql> update user set user.Host='%' where user.User='root';
mysql> flush privileges;
```



### REDIS

#### 1、安装

这⾥下载的是 redis-5.0.8.tar.gz 安装包，并将其直接放在了 root ⽬录下

```shell
wget http://download.redis.io/releases/redis-5.0.8.tar.gz
```

#### 2、解压安装包

在 /usr/local/ 下创建 redis ⽂件夹并进⼊

```shell
cd /usr/local/
mkdir redis
cd redis
```



将 Redis 安装包解压到 /usr/local/redis 中即可

```shell
tar zxvf /root/redis-5.0.8.tar.gz -C ./
```

解压完之后， /usr/local/redis ⽬录中会出现⼀个 redis-5.0.8 的⽬录



#### 3、编译并安装

```shell
cd redis-5.0.8/
make && make install
```



#### 4、将 REDIS 安装为系统服务并后台启动

进⼊ utils ⽬录，并执⾏如下脚本即可：

```shell
 cd utils/
 ./install_server.sh
```

此处我全部选择的默认配置即可，有需要可以按需定制



#### 5、查看REDIS服务启动情况

直接执⾏如下命令来查看Redis的启动结果：

```shell
systemctl status redis_6379.service
```



#### 6、启动REDIS客户端并测试

启动⾃带的 redis-cli 客户端，测试通过：

但是此时只能在本地访问，⽆法远程连接，因此还需要做部分设置



#### 7、设置允许远程连接

编辑 redis 配置⽂件

```shell
vi /etc/redis/6379.conf
```

将 bind 127.0.0.1 修改为 0.0.0.0，然后重启 Redis 服务即可：

```shell
systemctl restart redis_6379.service
```



#### 8、设置访问密码

编辑 redis 配置⽂件

```shell
vi /etc/redis/6379.conf
```

找到如下内容：

```shell
#requirepass foobared
```

去掉注释保存，重启 Redis 服务即可

```shell
systemctl restart redis_6379.service
```

这样后续的访问需要先输⼊密码认证通过⽅可

auth 密码



### RABBITMQ

#### 1、⾸先安装ERLANG环境

因为 RabbitMQ 需要 erlang 环境的⽀持，所以必须先安装 erlang 。

我们这⾥要安装的是 erlang-22.3.3-1.el7.x86_64.rpm ，所以⾸先执⾏如下命令来安装其对应的

yum repo ：

```shell
curl -s https://packagecloud.io/install/repositories/rabbitmq/erlang/script.rpm.sh | sudo bash
```

接下来执⾏如下命令正式安装 erlang 环境：

```shell
yum install erlang-22.3.3-1.el7.x86_64
```

接下来可以直接执⾏如下命令，测试 erlang 是否安装成功：

```shell
erl
```



#### 2、安装RABBITMQ

接下来正式开始安装 rabbitmq ，⾸先依然是安装其对应的 yum repo ：

```shell
curl -s https://packagecloud.io/install/repositories/rabbitmq/rabbitmq-server/script.rpm.sh | sudo bash
```

然后执⾏如下命令正式安装 rabbitmq ：

```shell
yum install rabbitmq-server-3.8.3-1.el7.noarch
```



#### 4、设置RABBITMQ开机启动

```shell
chkconfig rabbitmq-server on
```



#### 5、启动RABBITMQ服务

```shell
systemctl start rabbitmq-server.service
```



#### 6、开启WEB可视化管理插件

```shell
rabbitmq-plugins enable rabbitmq_management
```

浏览器输⼊： 你的服务器IP:15672，能看到⽹⻚登录⼊⼝即可

我们可以在后台先添加⼀个⽤户/密码对：

```shell
rabbitmqctl add_user sa 123456
rabbitmqctl set_user_tags sa administrator
```



### TOMCAT

#### 1、准备安装包

这⾥使⽤的是 8.5.55 版： apache-tomcat-8.5.55.tar.gz ，直接将其放在了 /root ⽬录下

```shell
wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.55/bin/apache-tomcat-8.5.55.tar.gz
```

#### 2、解压并安装

在 /usr/local/ 下创建 tomcat ⽂件夹并进⼊

```shell
cd /usr/local/
mkdir tomcat
cd tomcat
```

将 Tomcat 安装包解压到 /usr/local/tomcat 中即可

```shell
tar -zxvf /root/apache-tomcat-8.5.55.tar.gz -C ./
```

解压完之后， /usr/local/tomcat ⽬录中会出现⼀个 apache-tomcat-8.5.55 的⽬录



#### 3、启动TOMCAT

直接进 apache-tomcat-8.5.55 ⽬录，执⾏其中 bin ⽬录下的启动脚本即可

```shell
[root@localhost apache-tomcat-8.5.55]# cd bin/
[root@localhost bin]# ./startup.sh
```

这时候浏览器访问： 你的主机IP:8080 ，得到如下画⾯说明成功启动了



#### 4、配置快捷操作和开机启动

⾸先进⼊ /etc/rc.d/init.d ⽬录，创建⼀个名为 tomcat 的⽂件，并赋予执⾏权限

```shell
[root@localhost ~]# cd /etc/rc.d/init.d/
[root@localhost init.d]# touch tomcat
[root@localhost init.d]# chmod +x tomcat
```

接下来编辑 tomcat ⽂件，并在其中加⼊如下内容：

```shell
#!/bin/bash
#chkconfig:- 20 90
#description:tomcat
#processname:tomcat
TOMCAT_HOME=/usr/local/tomcat/apache-tomcat-8.5.55
case $1 in
 start) su root $TOMCAT_HOME/bin/startup.sh;;
 stop) su root $TOMCAT_HOME/bin/shutdown.sh;;
 *) echo "require start|stop" ;;
esac
```

这样后续对于Tomcat的开启和关闭只需要执⾏如下命令即可：

```shell
service tomcat start
service tomcat stop
```

最后加⼊开机启动即可：

```shell
chkconfig --add tomcat
chkconfig tomcat on
```



注意：

如果启动tomcat提示无法找到java环境，在以下文件开头增加配置：

```shell
vi /usr/local/tomcat/apache-tomcat-8.5.55/bin/catalina.sh

export JAVA_HOME=/usr/local/java/jdk1.8.0_161
export JRE_HOME=/usr/local/java/jdk1.8.0_161/jre
```



### NGINX

#### 1、⾸先安装包并解压

这⾥下载的是 nginx-1.17.10.tar.gz 安装包，并将其直接放在了 root ⽬录下，在 /usr/local/ 下创建 nginx ⽂件夹并进⼊

```shell
cd /usr/local/
mkdir nginx
cd nginx
```

将 Nginx 安装包解压到 /usr/local/nginx 中即可

```shell
tar zxvf /root/nginx-1.17.10.tar.gz -C ./
```

解压完之后， /usr/local/nginx ⽬录中会出现⼀个 nginx-1.17.10 的⽬录



#### 2、预先安装额外的依赖

```shell
yum -y install pcre-devel
yum -y install openssl openssl-devel
```



#### 3、编译安装NGINX

```shell
cd nginx-1.17.10
./configure
make && make install
```

安装完成后，Nginx的可执⾏⽂件位置位于

```shell
/usr/local/nginx/sbin/nginx
```

#### 4、启动NGINX

直接执⾏如下命令即可：

```shell
/usr/local/nginx/sbin/nginx
```

如果想停⽌Nginx服务，可执⾏：

```shell
/usr/local/nginx/sbin/nginx -s stop
```

如果修改了配置⽂件后想重新加载Nginx，可执⾏：

```shell
/usr/local/nginx/sbin/nginx -s reload
```

注意其配置⽂件位于：

```shell
/usr/local/nginx/conf/nginx.conf
```



### DOCKER

#### 1、安装DOCKER

```shell
yum install -y docker
```

静候安装完毕即可



#### 2、开启DOCKER服务

```shell
systemctl start docker.service
```

查看安装结果

```shell
docker version
```

#### 3、设置开机启动

```shell
systemctl enable docker.service
```

#### 4、配置DOCKER镜像下载加速

默认安装后的 Docker 环境在拉取 Docker 镜像时速度很慢，因此我们需要⼿动配置镜像加速源，提升获取 Docker 镜像的速度。

配置⽅法⾮常简单，直接编辑配置⽂件，在其中加⼊加速镜像源地址即可：

```shell
vi /etc/docker/daemon.json

{
 "registry-mirrors": ["http://hub-mirror.c.163.com"] 
}
```

⽐如这⾥使⽤的是 ⽹易 的加速源，其他像 阿⾥云 、 DaoCloud 这些也都提供加速源，按需选择即可。

加完加速地址后，重新加载配置⽂件，重启 docker 服务即可：

```shell
systemctl daemon-reload
systemctl restart docker.service
```

这样镜像加速器就配置完成了，后续下载 docker 镜像速度将⼤增。



### KUBERNETES集群部署

#### 概 述

Kubernetes集群的搭建⽅法其实有多种，⽐如我在之前的⽂章《利⽤K8S技术栈打造个⼈私有云（连载

之：K8S集群搭建）》中使⽤的就是⼆进制的安装⽅法。虽然这种⽅法有利于我们理解 k8s集群，但却过

于繁琐。⽽ kubeadm是 Kubernetes官⽅提供的⽤于快速部署Kubernetes集群的⼯具，其历经发展如

今已经⽐较成熟了，利⽤其来部署 Kubernetes集群可以说是⾮常好上⼿，操作起来也简便了许多，下

⾯详细叙述之。

---

#### 1、节点规划

本⽂准备部署⼀个 ⼀主两从 的 三节点 Kubernetes集群，整体节点规划如下表所示：

| 主机名     | **IP**         | ⻆⾊   |
| ---------- | -------------- | ------ |
| k8s-master | 192.168.31.108 | 主节点 |
| k8s-node-1 | 192.168.31.109 | 从节点 |
| k8s-node-2 | 192.168.31.60  | 从节点 |

下⾯介绍⼀下各个节点的软件版本：

- 操作系统： CentOS-7.4-64Bit

- Docker版本： 1.13.1

- Kubernetes版本： 1.13.1

所有节点都需要安装以下组件：

- Docker ：不⽤多说了吧

- kubelet ：运⾏于所有 Node上，负责启动容器和 Pod

- kubeadm ：负责初始化集群

- kubectl ： k8s命令⾏⼯具，通过其可以部署/管理应⽤ 以及CRUD各种资源



#### 2、准备⼯作

所有节点关闭防⽕墙

```shell
systemctl disable firewalld.service
systemctl stop firewalld.service
```

禁⽤SELINUX

```shell
setenforce 0
vi /etc/selinux/config
SELINUX=disabled
```

所有节点关闭 swap

```shell
swapoff -a
```

设置所有节点主机名

```shell
hostnamectl --static set-hostname k8s-master
hostnamectl --static set-hostname k8s-node-1
hostnamectl --static set-hostname k8s-node-2
```

所有节点 主机名/IP加⼊ hosts解析，编辑 /etc/hosts ⽂件，加⼊以下内容：

```shell
192.168.39.79 k8s-master
192.168.39.77 k8s-node-1
192.168.39.78 k8s-node-2
```



#### 3、组件安装

Docker安装（所有节点）

kubelet、kubeadm、kubectl安装（所有节点）

⾸先准备repo：

```shell
cat>>/etc/yum.repos.d/kubrenetes.repo<<EOF
[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
EOF
```

然后执⾏如下指令来进⾏安装

```shell
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX= disabled/' /etc/selinux/config
yum install -y kubelet kubeadm kubectl
systemctl enable kubelet && systemctl start kubelet
```



#### 4、MASTER节点配置

###### 4.1 初始化 k8s集群

为了应对⽹络不畅通的问题，我们国内⽹络环境只能提前⼿动下载相关镜像并重新打 tag :

```shell
docker pull mirrorgooglecontainers/kube-apiserver:v1.13.1
docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.1
docker pull mirrorgooglecontainers/kube-scheduler:v1.13.1
docker pull mirrorgooglecontainers/kube-proxy:v1.13.1
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd:3.2.24
docker pull coredns/coredns:1.2.6
docker pull registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64
docker tag mirrorgooglecontainers/kube-apiserver:v1.13.1
k8s.gcr.io/kube-apiserver:v1.13.1
docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.1
k8s.gcr.io/kube-controller-manager:v1.13.1
docker tag mirrorgooglecontainers/kube-scheduler:v1.13.1
k8s.gcr.io/kube-scheduler:v1.13.1
docker tag mirrorgooglecontainers/kube-proxy:v1.13.1 k8s.gcr.io/kube-proxy:v1.13.1
docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1
docker tag mirrorgooglecontainers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24
docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6
docker tag registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64
quay.io/coreos/flannel:v0.10.0-amd64
docker rmi mirrorgooglecontainers/kube-apiserver:v1.13.1 
docker rmi mirrorgooglecontainers/kube-controller-manager:v1.13.1 
docker rmi mirrorgooglecontainers/kube-scheduler:v1.13.1 
docker rmi mirrorgooglecontainers/kube-proxy:v1.13.1 
docker rmi mirrorgooglecontainers/pause:3.1 
docker rmi mirrorgooglecontainers/etcd:3.2.24 
docker rmi coredns/coredns:1.2.6
docker rmi registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64
```

然后再在 Master节点上执⾏如下命令初始化 k8s集群：

```shell
kubeadm init --kubernetes-version=v1.13.1 --apiserver-advertise-address 192.168.31.108 --pod-network-cidr=10.244.0.0/16
```

--kubernetes-version : ⽤于指定 k8s版本

--apiserver-advertise-address ：⽤于指定使⽤ Master的哪个network interface进⾏通

信，若不指定，则 kubeadm会⾃动选择具有默认⽹关的 interface

--pod-network-cidr ：⽤于指定Pod的⽹络范围。该参数使⽤依赖于使⽤的⽹络⽅案，本⽂

将使⽤经典的flannel⽹络⽅案。



执⾏命令后，控制台给出了如下所示的详细集群初始化过程：

```shell
[root@localhost ~]# kubeadm init --config kubeadm-config.yaml
W1224 11:01:25.408209 10137 strict.go:54] error unmarshaling
configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io",
Version:"v1beta1", Kind:"ClusterConfiguration"}: error unmarshaling
JSON: while decoding JSON: json: unknown field "\u00a0 podSubnet”
本文档在 Github开源项目：https://github.com/hansonwang99/JavaCollection 中已收录，
有详细自学编程学习路线、面试题和面经、编程资料及系列技术文章等，资源持续更新中...
本文档在 Github开源项目：https://github.com/hansonwang99/JavaCollection 中已收录，
有详细自学编程学习路线、面试题和面经、编程资料及系列技术文章等，资源持续更新中...
[init] Using Kubernetes version: v1.13.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of
your internet connection
[preflight] You can also perform this action in beforehand using
'kubeadm config images pull’
[kubelet-start] Writing kubelet environment file with flags to file
"/var/lib/kubelet/kubeadm-flags.env”
[kubelet-start] Writing kubelet configuration to file
"/var/lib/kubelet/config.yaml”
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder "/etc/kubernetes/pki”
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names
[localhost.localdomain localhost] and IPs [192.168.39.79 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names
[localhost.localdomain localhost] and IPs [192.168.39.79 127.0.0.1 ::1]
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names
[localhost.localdomain kubernetes kubernetes.default
kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs
[10.96.0.1 192.168.39.79]
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes”
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests”
[control-plane] Creating static Pod manifest for "kube-apiserver”
[control-plane] Creating static Pod manifest for "kube-controller￾manager”
[control-plane] Creating static Pod manifest for "kube-scheduler”
[etcd] Creating static Pod manifest for local etcd in
"/etc/kubernetes/manifests”
[wait-control-plane] Waiting for the kubelet to boot up the control
plane as static Pods from directory "/etc/kubernetes/manifests". This
can take up to 4m0s
[apiclient] All control plane components are healthy after 24.005638
seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm￾config" in the "kube-system” Namespace
本文档在 Github开源项目：https://github.com/hansonwang99/JavaCollection 中已收录，
有详细自学编程学习路线、面试题和面经、编程资料及系列技术文章等，资源持续更新中...
本文档在 Github开源项目：https://github.com/hansonwang99/JavaCollection 中已收录，
有详细自学编程学习路线、面试题和面经、编程资料及系列技术文章等，资源持续更新中...
0x02. 配置 kubectl
在 Master上⽤ root⽤户执⾏下列命令来配置 kubectl：
[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kubesystem with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information
"/var/run/dockershim.sock" to the Node API object
"localhost.localdomain" as an annotation
[mark-control-plane] Marking the node localhost.localdomain as controlplane by adding the label "node-role.kubernetes.io/master=''”
[mark-control-plane] Marking the node localhost.localdomain as controlplane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 26uprk.t7vpbwxojest0tvq
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap,
RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens
to post CSRs in order for nodes to get long term certificate
credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover
controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation
for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kubepublic” namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy
Your Kubernetes master has initialized successfully!
To start using your cluster, you need to run the following as a regular
user:
 mkdir -p $HOME/.kube
 sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 sudo chown $(id -u):$(id -g) $HOME/.kube/config
You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed
at:
 https://kubernetes.io/docs/concepts/cluster-administration/addons/
You can now join any number of machines by running the following on
each node
as root:
 kubeadm join 192.168.39.79:6443 --token 26uprk.t7vpbwxojest0tvq --
discovery-token-ca-cert-hash
sha256:028727c0c21f22dd29d119b080dcbebb37f5545e7da1968800140ffe225b0123
[root@localhost ~]#
```



##### 4.2配置 kubectl

在 Master上⽤ root⽤户执⾏下列命令来配置 kubectl：

```shell
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /etc/profile
source /etc/profile
echo $KUBECONFIG
```



##### 4.3安装Pod⽹络

安装 Pod⽹络是 Pod之间进⾏通信的必要条件，k8s⽀持众多⽹络⽅案，这⾥我们依然选⽤经典的flannel⽅案

⾸先设置系统参数：

```shell
sysctl net.bridge.bridge-nf-call-iptables=1
```

然后在 Master节点上执⾏如下命令：

```shell
kubectl apply -f kube-flannel.yaml
```

kube-flannel.yaml ⽂件下载地址：

https://github.com/hansonwang99/JavaCollection/blob/master/docs/kubernetes/kube-flannel.yaml

⼀旦 Pod⽹络安装完成，可以执⾏如下命令检查⼀下 CoreDNS Pod此刻是否正常运⾏起来了，⼀旦其

正常运⾏起来，则可以继续后续步骤

```shell
kubectl get pods --all-namespaces -o wide
```

同时我们可以看到主节点已经就绪： kubectl get nodes



#### 5、添加 SLAVE节点

在两个 Slave节点上分别执⾏如下命令来让其加⼊Master上已经就绪了的 k8s集群：

```shell
kubeadm join --token <token> <master-ip>:<master-port> --discoverytoken-ca-cert-hash sha256:<hash>
```

如果 token忘记，则可以去 Master上执⾏如下命令来获取：

```shell
kubeadm token list
```

上述kubectl join命令的执⾏结果如下：

```shell
[root@localhost ~]# kubeadm join 192.168.39.79:6443 --token
yndddp.oamgloerxuune80q --discovery-token-ca-cert-hash
sha256:7a45c40b5302aba7d8b9cbd3afc6d25c6bb8536dd6317aebcd2909b0427677c8
[preflight] Running pre-flight checks
[discovery] Trying to connect to API Server "192.168.39.79:6443”
[discovery] Created cluster-info discovery client, requesting info from
"https://192.168.39.79:6443”
[discovery] Requesting info from "https://192.168.39.79:6443" again to
validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS
certificate validates against pinned roots, will use API Server
"192.168.39.79:6443”
[discovery] Successfully established connection with API Server
"192.168.39.79:6443”
[join] Reading configuration from the cluster…
[join] FYI: You can look at this config file with 'kubectl -n kubesystem get cm kubeadm-config -oyaml’
[kubelet] Downloading configuration for the kubelet from the "kubeletconfig-1.13" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file
"/var/lib/kubelet/config.yaml”
[kubelet-start] Writing kubelet environment file with flags to file
"/var/lib/kubelet/kubeadm-flags.env”
[kubelet-start] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap…
[patchnode] Uploading the CRI Socket information
"/var/run/dockershim.sock" to the Node API object
"localhost.localdomain" as an annotation
This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was
received.
* The Kubelet was informed of the new secure connection details.
Run 'kubectl get nodes' on the master to see this node join the
cluster.
```



#### 6、效果验证

查看节点状态

```shell
kubectl get nodes
```

查看所有 Pod状态

```shell
kubectl get pods --all-namespaces -o wide
```

好了，集群现在已经正常运⾏了，接下来看看如何正常的拆卸集群。



#### 7、拆卸集群

⾸先处理各节点：

```shell
kubectl drain <node name> --delete-local-data --force --ignore -daemonsets
kubectl delete node <node name>
```

⼀旦节点移除之后，则可以执⾏如下命令来重置集群：

```shell
kubeadm reset
```



#### 8、安装 DASHBOARD

就像给elasticsearch配⼀个可视化的管理⼯具⼀样，我们最好也给 k8s集群配⼀个可视化的管理⼯具，便于管理集群。

因此我们接下来安装 v1.10.0 版本的 kubernetes-dashboard，⽤于集群可视化的管理。

- ⾸先⼿动下载镜像并重新打标签：（所有节点）

```shell;
docker pull registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetesdashboard-amd64:v1.10.0
docker tag registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetesdashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0
docker image rm registry.cn-qingdao.aliyuncs.com/wangxiaoke/kubernetesdashboard-amd64:v1.10.0
```

- 安装 dashboard：

```shell
kubectl create -f dashboard.yaml
```

dashboard.yaml ⽂件下载地址：

https://github.com/hansonwang99/JavaCollection/blob/master/docs/kubernetes/dashboard.yaml

- 查看 dashboard的 pod是否正常启动，如果正常说明安装成功:

```shell
 kubectl get pods --namespace=kube-system
```

```shell
[root@k8s-master ~]# kubectl get pods --namespace=kube-system
NAME READY STATUS RESTARTS 
AGE
coredns-86c58d9df4-4rds2 1/1 Running 0 
81m
coredns-86c58d9df4-rhtgq 1/1 Running 0 
81m
etcd-k8s-master 1/1 Running 0 
80m
kube-apiserver-k8s-master 1/1 Running 0 
80m
kube-controller-manager-k8s-master 1/1 Running 0 
80m
kube-flannel-ds-amd64-8qzpx 1/1 Running 0 
78m
kube-flannel-ds-amd64-jvp59 1/1 Running 0 
77m
kube-flannel-ds-amd64-wztbk 1/1 Running 0 
78m
本文档在 Github开源项目：https://github.com/hansonwang99/JavaCollection 中已收录，
有详细自学编程学习路线、面试题和面经、编程资料及系列技术文章等，资源持续更新中...
本文档在 Github开源项目：https://github.com/hansonwang99/JavaCollection 中已收录，
有详细自学编程学习路线、面试题和面经、编程资料及系列技术文章等，资源持续更新中...
kube-proxy-crr7k 1/1 Running 0 
81m
kube-proxy-gk5vf 1/1 Running 0 
78m
kube-proxy-ktr27 1/1 Running 0 
77m
kube-scheduler-k8s-master 1/1 Running 0 
80m
kubernetes-dashboard-79ff88449c-v2jnc 1/1 Running 0 
21s
```

- 查看 dashboard的外⽹暴露端⼝

```shell
kubectl get service --namespace=kube-system
```

```shell
NAME TYPE CLUSTER-IP EXTERNAL-IP 
PORT(S) AGE
kube-dns ClusterIP 10.96.0.10 <none> 
53/UDP,53/TCP 5h38m
kubernetes-dashboard NodePort 10.99.242.186 <none> 
443:31234/TCP 14
```

- ⽣成私钥和证书签名：

```shell
openssl genrsa -des3 -passout pass:x -out dashboard.pass.key 2048
openssl rsa -passin pass:x -in dashboard.pass.key -out dashboard.key
rm dashboard.pass.key
openssl req -new -key dashboard.key -out dashboard.csr【如遇输⼊，⼀路回⻋
即可】
```

- ⽣成SSL证书：

```shell
openssl x509 -req -sha256 -days 365 -in dashboard.csr -signkey
dashboard.key -out dashboard.crt
```

- 然后将⽣成的 dashboard.key 和 dashboard.crt 置于路径 /home/share/certs 下，该路

  径会配置到下⾯即将要操作的dashboard-user-role.yaml ⽂件中

  

- 创建 dashboard⽤户

```shell
 kubectl create -f dashboard-user-role.yaml
```

dashboard-user-role.yaml ⽂件下载地址：

https://github.com/hansonwang99/JavaCollection/blob/master/docs/kubernetes/dashboard-user-role.yaml

- 获取登陆token

```shell
kubectl describe secret/$(kubectl get secret -nkube-system |grep
admin|awk '{print $1}') -nkube-system
```

```shell
[root@k8s-master ~]# kubectl describe secret/$(kubectl get secret -
nkube-system |grep admin|awk '{print $1}') -nkube-system
Name: admin-token-9d4vl
Namespace: kube-system
Labels: <none>
Annotations: kubernetes.io/service-account.name: admin
 kubernetes.io/service-account.uid: a320b00f-07ed-11e9-
93f2-000c2978f207
Type: kubernetes.io/service-account-token
Data
====
ca.crt: 1025 bytes
namespace: 11 bytes
token: 
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2
NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlL
XN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJh
ZG1pbi10b2tlbi05ZDR2bCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2Vydml
jZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bn
Qvc2VydmljZS1hY2NvdW50LnVpZCI6ImEzMjBiMDBmLTA3ZWQtMTFlOS05M2YyLTAwMGMyO
Tc4ZjIwNyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1p
biJ9.WbaHxBfZEd0SvJwA9V_vGUe8jPMUHjKlkT7MWJ4JcQldRFY8Tdpv5GKCY25JsvT_GM3ob303r0yE
6vjQdKna7EfQNO_Wb2j1Yu5UvZnWw52HhNudHNOVL_fFRKxkSVjAILA_C_HvW6aw6TG5h7z
HARgl71I0LpW1VESeHeThipQ-pkt-Dr1jWcpPgE39cwxSgi-
5qY4ssbyYBc2aPYLsqJibmEKUhwmyOheF4Lxpg7E3SQEczsig2HjXpNtJizCu0kPyiR4qbbsusulHkdgjhmD9_XWP9k0BzgutXWteV8Iqe4-uuRGHZAxgutCvaL5qENv4OAlaArlZqSgkNWw
```

token既然⽣成成功，接下来就可以打开浏览器，输⼊ token来登录进集群管理⻚⾯：

测试